

# Generator

通过列表生成式，我们可以直接创建一个列表。但是，受到内存限制，列表容量肯定是有限的。所以，如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？这样就不必创建完整的list，从而节省大量的空间。**在Python中，这种一边循环一边计算的机制，称为生成器：generator。**

要创建一个generator。第一种方法，把一个列表生成式的`[]`改成`()`：

```
>>> L = [x * x for x in range(10)]				
>>> L											# L是一个list
[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
>>> g = (x * x for x in range(10))
>>> g
<generator object <genexpr> at 0x1022ef630>		# g是一个generator
```

> 如果要一个一个打印出来，可以通过`next()`函数获得generator的下一个返回值：

> ```
> >>> next(g)					# generator保存的是算法，每次调用next(g)，就计算出g的下一个元素的值
> 0
> >>> next(g)
> 1
> >>> next(g)
> 4
> >>> next(g)
> 9
> >>> next(g)
> 16
> >>> next(g)
> 25
> >>> next(g)
> 36
> >>> next(g)
> 49
> >>> next(g)
> 64
> >>> next(g)
> 81
> >>> next(g)
> Traceback (most recent call last):				# 没有更多的元素时，抛出StopIteration的错误。
>   File "<stdin>", line 1, in <module>
> StopIteration
> ```

正确的方法是使用 `for` 循环，因为generator也是可迭代对象：

```
>>> g = (x * x for x in range(10))
>>> for n in g:
...     print(n)
... 
0
1
4
9
16
25
36
49
64
81
```



# Generator Functions and Expressions

two language constructs delay result creation whenever possible in user-defined operations:

- Generator functions (available since 2.3) are coded as normal `def` statements, but use `yield` statements to return results one at a time, suspending and resuming their state between each.


- Generator expressions (available since 2.4) are similar to the list comprehensions of the prior section, but they return an object that produces results on demand instead of building a result list.

Because neither constructs a result list all at once, they save memory space and allow computation time to be split across result requests.



## Generator Functions: yield Versus return

functions that may send back a value and later be resumed, picking up where they left off. Such functions, available in both Python 2.X and 3.X, are known as **generator** functions because they generate a sequence of values over time.

> Generator functions are like normal functions in most respects, and in fact are coded with normal `def` statements. However, when created, they are compiled specially into an object that supports the iteration protocol. And when called, they don’t return a result: they return a result generator that can appear in any iteration context.

### State suspension

generator functions automatically suspend and resume their execution and state around the point of value generation. Because of that, they are often a useful alternative to both computing an entire series of values up front and manually saving and restoring state in classes. The state that generator functions retain when they are suspended includes both their code location, and their entire local scope. Hence, their local variables retain information between results, and make it available when the functions are resumed.

The chief code difference between generator and normal functions is that a generator yields a value, rather than returning one—the yield statement suspends the function and sends a value back to the caller, but retains enough state to enable the function to resume from where it left off. When resumed, the function continues execution immediately after the last `yield` run. From the function’s perspective, this allows its code to produce a series of values over time, rather than computing them all at once and sending them back in something like a list.

### Iteration protocol integration

generator functions are closely bound up with the notion of the iteration protocol in Python.

iterator objects define a `__next__` method (`next` in 2.X), which either returns the next item in the iter-ation, or raises the special `StopIteration` exception to end the iteration. An iterable object’s iterator is fetched initially with the `iter` built-in function, though this step is a no-op for objects that are their own iterator.

To support this protocol, functions containing a `yield` statement are compiled specially as **generators**—they are not normal functions, but rather are built to return an object with the expected iteration protocol methods. When later called, they return a generator object that supports the iteration interface with an automatically created method named `__next__` to start or resume execution.

Generator functions may also have a `return` statement that, along with falling off the end of the `def` block, simply terminates the generation of values—technically, by raising a `StopIteration` exception after any normal function exit actions. From the caller’s perspective, the generator’s `__next__` method resumes the function and runs until either the next `yield` result is returned or a `StopIteration` is raised.

The net effect is that generator functions, coded as `def` statements containing `yield` statements, are automatically made to support the iteration object protocol and thus may be used in any iteration context to produce results over time and on demand.

### Generator functions in action

The following code defines a generator function that can be used to generate the squares of a series of numbers over time:

```
>>> def gensquares(N):
...     for i in range(N):
...         yield i ** 2		# Resume here later
```

This function yields a value, and so returns to its caller, each time through the loop; when it is resumed, its prior state is restored, including the last values of its variables i and N, and control picks up again immediately after the `yield` statement. For example, when it’s used in the body of a `for` loop, the first iteration starts the function and gets its first result; thereafter, control returns to the function after its `yield` statement each time through the loop:

```
>>> for i in gensquares(5):		# Resume the function
...     print(i, end=' : ')		# Print last yielded value
... 
0 : 1 : 4 : 9 : 16 : 
```

To end the generation of values, functions either use a `return` statement with no value or simply allow control to fall off the end of the function body.

to see what is going on inside the for, call the generator function directly:

```
>>> x = gensquares(4)
>>> x 
<generator object gensquares at 0x000000000292CA68>
```

the `next(X)` built-in calls an object’s `X.__next__()` method for us in 3.X (and `X.next()` in 2.X):

```
>>> next(x)			# Same as x.__next__() in 3.X
0
>>> next(x)			# Use x.next() or next() in 2.X
1
>>> next(x)
4
>>> next(x)
9
>>> next(x)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
StopIteration
```

`for` loops (and other iteration contexts) work with generators in the same way—by calling the `__next__` method repeatedly, until an exception is caught. For a generator, the result is to produce yielded values over time. If the object to be iterated over does not support this protocol, `for` loops instead use the indexing protocol to iterate.

generators are their own iterator, supporting just one active iteration scan. To put that another way generators return themselves for `iter`, because they support `next` directly

```
>>> y = gensquares(5)		# Returns a generator which is its own iterator
>>> iter(y) is y			# iter() is not required: a no-op here
True
>>> next(y)					# Can run next()immediately
0
```

### Why generator functions?

build the list of yielded values all at once:

```
>>> def buildsquares(n):
...     res = []
...     for i in range(n): res.append(i ** 2)
...     return res
... 
>>> for x in buildsquares(5): print(x, end = ' : ')
... 
0 : 1 : 4 : 9 : 16 : 
```

use any of the for loop, map, or list comprehension techniques:

```
>>> for x in [n ** 2 for n in range(5)]:
...     print(x, end=' : ')
... 
0 : 1 : 4 : 9 : 16 : 
>>> 
>>> for x in map((lambda n: n ** 2), range(5)):
...     print(x, end=' : ')
... 
0 : 1 : 4 : 9 : 16 : 
```

They can operate on and return any type of object, and as iterables may appear in any of  iteration contexts, including `tuple` calls, enumerations, and dictionary comprehensions:

```
>>> def ups(line):
...     for sub in line.split(','):		# Substring generator
...         yield sub.upper()
... 

>>> tuple(ups('aaa,bbb,ccc'))			# All iteration contexts
('AAA', 'BBB', 'CCC')

>>> {i: s for (i, s) in enumerate(ups('aaa,bbb,ccc'))} 
{0: 'AAA', 1: 'BBB', 2: 'CCC'}
```

### Extended generator function protocol: send versus next

In Python 2.5, a `send` method was added to the generator function protocol. The send method advances to the next item in the series of results, just like `__next__`, but also provides a way for the caller to communicate with the generator, to affect its operation.

`yield` is now an expression form that returns the item passed to send, not a statement (though it can be called either way—as `yield X`, or `A = (yield X)`).The expression must be enclosed in parentheses unless it’s the only item on the right side of the assignment statement. For example, `X = yield Y` is OK, as is `X = (yield Y) + 42`.

When this extra protocol is used, values are sent into a generator `G` by calling `G.send(value)`. The generator’s code is then resumed, and the `yield` expression in the generator returns the value passed to `send`. If the regular `G.__next__()` method (or its `next(G)` equivalent) is called to advance, the `yield` simply returns `None`.

example:

```
>>> def gen():
...     for i in range(10):
...         X = yield i
...         print(X)
... 
>>> G = gen()
>>> next(G)				# Must call next() first, to start generator
0
>>> G.send(77)			# Advance, and send value to yield expression
77
1
>>> G.send(88)
88
2
>>> next(G)				# next() and X.__next__() send None
None
3
```

the `__next__` method applies to all iterable objects—both built-in types and user-defined classes.



## Generator Expressions: Iterables Meet Comprehensions

In both Python 2.X and 3.X, the notions of iterables and list comprehensions are combined in a new tool: generator expressions.

> generator expressions are just like normal list comprehensions, and support all their syntax —including if filters and loop nesting—but they are enclosed in parentheses instead of square brackets (like tuples, their enclosing parentheses are often optional):

```
>>> [x ** 2 for x in range(4)]	# List comprehension: build a list
[0, 1, 4, 9]
>>> (x ** 2 for x in range(4))	# Generator expression: make an iterable
<generator object <genexpr> at 0x101139fc0>
```

at least on a functionality basis, coding a list comprehension is essentially the same as wrapping a generator expression in a `list` built-in call to force it to produce all its results in a list at once:

```
>>> list(x ** 2 for x in range(4)) 	# List comprehension equivalence
[0, 1, 4, 9]
```

generator expressions are very different: instead of building the result list in memory, they return a generator object—an automatically created iterable. This iterable object in turn supports the iteration protocol to yield one piece of the result list at a time in any iteration context. The iterable object also retains gen-erator state while active—the variable x in the preceding expressions, along with the generator’s code location.

```
>>> G = (x ** 2 for x in range(4))
>>> iter(G) is G			# iter(G) optional: __iter__ returns self
True
>>> next(G)					# Generator objects: automatic methods
0
>>> next(G)
1
>>> next(G)
4
>>> next(G)
9
>>> next(G)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
StopIteration
>>> G
<generator object <genexpr> at 0x1011dc048>
```

for loops trigger the `next` iterator for us automatically: (every iteration context does this)

```
>>> for num in (x ** 2 for x in range(4)):	# Calls next() automatically
    	print('%s, %s' % (num, num / 2.0)) 
0, 0.0 
1, 0.5 
4, 2.0 
9, 4.5
```

parentheses are not required around a generator expression that is the sole item already enclosed in parentheses used for other purposes. Parentheses are required in all other cases, however, even if they seem extra, as in the second call to `sorted` that follows:

```
>>> sum(x ** 2 for x in range(4)) 			# Parens optional
14
>>> sorted(x ** 2 for x in range(4)) 		# Parens optional
[0, 1, 4, 9]
>>> sorted((x ** 2 for x in range(4)), reverse=True) 	# Parens optional
[9, 4, 1, 0]
```

### Why generator expressions?

Just like generator functions, generator expressions are a memory-space optimization —they do not require the entire result list to be constructed all at once, as the square-bracketed list comprehension does. Also like generator functions, they divide the work of results production into smaller time slices—they yield results in piecemeal fashion, instead of making the caller wait for the full set to be created in a single call.

generator expressions may also run slightly slower than list comprehensions in practice, so they are probably best used only for very large result sets, or applications that cannot wait for full results generation. A more authoritative statement about performance,

### Generator expressions versus map

generator expressions often are equivalent to 3.X map calls, because both generate result items on request.

```
>>> list(map(abs, (−1, −2, 3, 4))) 				# Map function on tuple
[1, 2, 3, 4]
>>> list(abs(x) for x in (−1, −2, 3, 4)) 		# Generator expression
[1, 2, 3, 4]
>>> list(map(lambda x: x * 2, (1, 2, 3, 4))) 	# Nonfunction case
[2, 4, 6, 8]
>>> list(x * 2 for x in (1, 2, 3, 4)) 			# Simpler as generator?
[2, 4, 6, 8]
```



```
>>> line = 'aaa,bbb,ccc'
>>> ''.join([x.upper() for x in line.split(',')]) # Makes a pointless list
'AAABBBCCC'

>>> ''.join(x.upper() for x in line.split(',')) 	# Generates results
'AAABBBCCC'
>>> ''.join(map(str.upper, line.split(','))) 		# Generates results
'AAABBBCCC'

>>> ''.join(x * 2 for x in line.split(',')) 		# Simpler as generator?
'aaaaaabbbbbbcccccc'
>>> ''.join(map(lambda x: x * 2, line.split(','))) 
'aaaaaabbbbbbcccccc'
```

Both map and generator expressions can also be arbitrarily nested,

```
>>> [x * 2 for x in [abs(x) for x in (−1, −2, 3, 4)]] 
[2, 4, 6, 8]										# Nested comprehensions

>>> list(map(lambda x: x * 2, map(abs, (−1, −2, 3, 4)))) 
[2, 4, 6, 8]										# Nested maps

>>> list(x * 2 for x in (abs(x) for x in (−1, −2, 3, 4))) 
[2, 4, 6, 8]										# Nested generators
```

combine operations, the generators do so without making multiple temporary lists.

```
>>> import math
>>> list(map(math.sqrt, (x ** 2 for x in range(4)))) 
[0.0, 1.0, 2.0, 3.0]								# Nested combinations
```

flat is generally better than nested:

```
>>> list(abs(x) * 2 for x in (−1, −2, 3, 4)) 		# Unnested equivalents
[2, 4, 6, 8]
>>> list(math.sqrt(x ** 2) for x in range(4)) 		# Flat is often better
[0.0, 1.0, 2.0, 3.0]
>>> list(abs(x) for x in (−1, 0, 1)) 
[1, 0, 1]
```

### Generator expressions versus filter

Because `filter` is an iterable in 3.X that generates its results on request, a generator expression with an `if` clause is operationally equivalent (in 2.X, `filter` produces a temporary list that the generator does not, but the code comparisons again apply).

```
>>> line = 'aa bbb c'
>>> ''.join(x for x in line.split() if len(x) > 1) 			# Generator with 'if'
'aabbb'
>>> ''.join(filter(lambda x: len(x) > 1, line.split())) 	# Similar to filter
'aabbb'
```

As for list comprehen-sions, though, adding processing steps to filter results requires a `map` too, which makes filter noticeably more complex than a generator expression:

```
>>> ''.join(x.upper() for x in line.split() if len(x) > 1) 
'AABBB'
>>> ''.join(map(str.upper, filter(lambda x: len(x) > 1, line.split()))) 
'AABBB'
```

a statement-based equivalent to a generator expression, though it sometimes renders substantially more code:

```
>>> ''.join(x.upper() for x in line.split() if len(x) > 1) 
'AABBB'

>>> res = ''
>>> for x in line.split():		# Statement equivalent? 
		if len(x) > 1:			# This is also a join
			res += x.upper()

>>> res 
'AABBB'
```



## Generator Functions Versus Generator Expressions

- **Generator functions**

  >  A function `def` statement that contains a `yield` statement is turned into a generator function. When called, it returns a new generator object with automatic retention of local scope and code position; an automatically created `__iter__` method that simply returns itself; and an automatically created `__next__` method (`next` in 2.X) that starts the function or resumes it where it last left off, and raises `StopIteration` when finished producing results.


- **Generator expressions** 

  > A comprehension expression enclosed in parentheses is known as a generator expression. When run, it returns a new generator object with the same automatically created method interface and state retention as a generator function call’s results —with an `__iter__` method that simply returns itself; and a `__next__` method (`next` in 2.X) that starts the implied loop or resumes it where it last left off, and raises `StopIteration` when finished producing results.

**Example 1 **, repeats each character in a string four times:

```
>>> G = (c * 4 for c in 'SPAM')			# Generator expression
>>> list(G) 							# Force generator to produce all results
['SSSS', 'PPPP', 'AAAA', 'MMMM']	
```

The equivalent generator function

```
>>> def timesfour(S):					# Generator function
...     for c in S:
...         yield c * 4
... 
>>> G = timesfour('spam')
>>> list(G)								# Iterate automatically
['ssss', 'pppp', 'aaaa', 'mmmm']
```

Both expressions and functions support both automatic and manual iteration—the prior `list` call iterates automatically, and the following iterate manually:

```
>>> G = (c * 4 for c in 'SPAM')
>>> I = iter(G)						# Iterate manually (expression)
>>> next(I) 
'SSSS'
>>> next(I) 
'PPPP'

>>> G = timesfour('spam')
>>> I = iter(G)						# Iterate manually (function)
>>> next(I) 
'ssss'
>>> next(I) 
'pppp'
```

**Example 2 :**

```
>>> line = 'aa bbb c'

>>> ''.join(x.upper() for x in line.split() if len(x) > 1) '		# Expression
AABBB'

>>> def gensub(line):				# Function
		for x in line.split(): 
			if len(x) > 1: 
				yield x.upper()

>>> ''.join(gensub(line)) 			# But why generate?
'AABBB'
```



## Generators Are Single-Iteration Objects

both generator functions and generator expressions are their own iterators and thus support just one active iteration. you can’t have multiple iterators of either positioned at different locations in the set of results.

a generator’s iterator is the generator itself.

```
>>> G = (c * 4 for c in 'SPAM')
>>> iter(G) is G 					# My iterator is myself: G has __next__
True
```

If you iterate over the results stream manually with multiple iterators, they will all point to the same position:

```
>>> G = (c * 4 for c in 'SPAM')		# Make a new generator
>>> I1 = iter(G)					# Iterate manually
>>> next(I1) 
'SSSS'
>>> next(I1) 
'PPPP'
>>> I2 = iter(G)					# Second iterator at same position!
>>> next(I2) 
'AAAA'
```

once any iteration runs to completion, all are exhausted—we have to make a new generator to start again:

```
>>> list(I1) 						# Collect the rest of I1's items
['MMMM']
>>> next(I2) 						# Other iterators exhausted too
StopIteration

>>> I3 = iter(G)					# Ditto for new iterators
>>> next(I3) 
StopIteration

>>> I3 = iter(c * 4 for c in 'SPAM')	# New generator to start over
>>> next(I3) 
'SSSS'
```

The same holds true for generator functions—the following `def` statement-based equivalent supports just one active iterator and is exhausted after one pass:

```
>>> def timesfour(S):
		for c in S: 
			yield c * 4
>>> G = timesfour('spam')			# Generator functions work the same way
>>> iter(G) is G 
True
>>> I1, I2 = iter(G), iter(G)
>>> next(I1) 
'ssss'
>>> next(I1) 
'pppp'
>>> next(I2) 						# I2 at same position as I1
'aaaa'
```

This is different from the behavior of some built-in types, which support multiple iter-ators and passes and reflect their in-place changes in active iterators:

```
>>> L = [1, 2, 3, 4]
>>> I1, I2 = iter(L), iter(L)
>>> next(I1) 
1
>>> next(I1) 
2
>>> next(I2) 					# Lists support multiple iterators
1
>>> del L[2:]					# Changes reflected in iterators
>>> next(I1) 
StopIteration
```

**if you wish to scan a generator’s values multiple times, you must either create a new generator for each scan or build a rescannable list out of its values—a single generator’s values will be consumed and exhausted after a single pass.**



## The Python 3.3 yield from Extension

Python 3.3 introduces extended syntax for the yield statement that allows delegation to a subgenerator with a from generator clause. In simple cases, it’s the equivalent to a yielding for loop—the list here in the following forces the generator to produce all its values, and the comprehension in parentheses is a generator expression,

```
>>> def both(N):
		for i in range(N): yield i 
		for i in (x ** 2 for x in range(N)): yield i

>>> list(both(5)) 
[0, 1, 2, 3, 4, 0, 1, 4, 9, 16]
```

The new 3.3 syntax makes this arguably more concise and explicit, and supports all the usual generator usage contexts:

```
>>> def both(N):
		yield from range(N) 
		yield from (x ** 2 for x in range(N))

>>> list(both(5)) 
[0, 1, 2, 3, 4, 0, 1, 4, 9, 16]

>>> ' : '.join(str(i) for i in both(5)) 
'0 : 1 : 2 : 3 : 4 : 0 : 1 : 4 : 9 : 16'
```

this is only available in 3.3 and later



## Generation in Built-in Types, Tools, and Classes

**Example 1**, dictionaries are iterables with iterators that produce keys on each iteration:

```
>>> D = {'a':1, 'b':2, 'c':3}
>>> x = iter(D)
>>> next(x) 
'c'
>>> next(x) 
'b'
```

### Generators and library tools: Directory walkers

the standard directory walker—which at each level of a tree yields a tuple of the current directory, its subdirectories, and its files:

```
>>> import os
>>> for (root, subs, files) in os.walk('.'):		# Directory walk generator
		for name in files:							# A Python 'find' operation
			if name.startswith('call'): 
				print(root, name)

. callables.py 
.\dualpkg callables.py
```

By yielding results as it goes, the walker does not require its clients to wait for an entire tree to be scanned.

```
>>> G = os.walk(r'C:\code\pkg')
>>> iter(G) is G 					# Single-scan iterator: iter(G) optional
True
>>> I = iter(G)
>>> next(I) 
('C:\\code\\pkg', ['__pycache__'], ['eggs.py', 'eggs.pyc', 'main.py', ...etc...])
>>> next(I) 
('C:\\code\\pkg\\__pycache__', [], ['eggs.cpython-33.pyc', ...etc...])
>>> next(I) 
StopIteration
```

### Generators and function application

starred arguments can unpack an iterable into individual arguments.

```
>>> def f(a, b, c): print('%s, %s, and %s' % (a, b, c))
>>> f(0, 1, 2)											# Normal positionals
0, 1, and 2
>>> f(*range(3)) 						# Unpack range values: iterable in 3.X
0, 1, and 2
>>> f(*(i for i in range(3))) 			# Unpack generator expression values
0, 1, and 2
```

This applies to dictionaries and views too (though `dict.values` is also a list in 2.X, and order is arbitrary when passing values by position):

```
>>> D = dict(a='Bob', b='dev', c=40.5); D 
{'b': 'dev', 'c': 40.5, 'a': 'Bob'}
>>> f(a='Bob', b='dev', c=40.5) 			# Normal keywords 
Bob, dev, and 40.5
>>> f(**D) 									# Unpack dict: key=value 
Bob, dev, and 40.5
>>> f(*D) 									# Unpack keys iterator 
b, c, and a
>>> f(*D.values()) 							# Unpack view iterator: iterable in 3.X 
dev, 40.5, and Bob
```

the following three forms equivalent

```
>>> for x in 'spam': print(x.upper(), end=' ') 
S P A M

>>> list(print(x.upper(), end=' ') for x in 'spam') 
S P A M [None, None, None, None]

>>> print(*(x.upper() for x in 'spam')) 
S P A M
```



## Example: Generating Scrambled Sequences

### Scrambling sequences

```
>>> L, S = [1, 2, 3], 'spam'
>>> for i in range(len(S)):			# For repeat counts 0..3 
		S = S[1:] + S[:1] 			# Move front item to the end
		print(S, end=' ')

pams amsp mspa spam

>>> for i in range(len(L)):
		L = L[1:] + L[:1] 			# Slice so any sequence type works
		print(L, end=' ')

[2, 3, 1] [3, 1, 2] [1, 2, 3]
----------
```

### Simple functions

```
>>> def scramble(seq):
		res = [] 
		for i in range(len(seq)): 
			res.append(seq[i:] + seq[:i]) 
		return res

>>> scramble('spam') 
['spam', 'pams', 'amsp', 'mspa']

>>> def scramble(seq):
		return [seq[i:] + seq[:i] for i in range(len(seq))]

>>> scramble('spam') 
['spam', 'pams', 'amsp', 'mspa']

>>> for x in scramble((1, 2, 3)): 
		print(x, end=' ')

(1, 2, 3) (2, 3, 1) (3, 1, 2)
```

### Generator functions

```
>>> def scramble(seq):
		for i in range(len(seq)):
			seq = seq[1:] + seq[:1] 		# Generator function
			yield seq						# Assignments work here
			
>>> def scramble(seq):
		for i in range(len(seq)): 			# Generator function
			yield seq[i:] + seq[:i]			# Yield one item per iteration

>>> list(scramble('spam')) 					# list()generates all results
['spam', 'pams', 'amsp', 'mspa']
>>> list(scramble((1, 2, 3))) 				# Any sequence type works
[(1, 2, 3), (2, 3, 1), (3, 1, 2)]
>>>
>>> for x in scramble((1, 2, 3)): 			# for loops generate results
		print(x, end=' ')

(1, 2, 3) (2, 3, 1) (3, 1, 2)
```

### Generator expressions